# -*- coding: utf-8 -*-
"""Diabetes_Prediction

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jprr24zegR3XLF4AWofY_v2HquUwSLLA
"""

import pandas as pd

diabetes = pd.read_csv('diabetes_data.csv')
diabetes.head(5)

diabetes.tail(5)

diabetes.describe()

diabetes.describe(include='object')

diabetes.info()

import pandas as pd
import matplotlib.pyplot as plt
import seaborn as sns

# Assuming 'diabetes' is your DataFrame
# Select the numerical columns
numerical_cols = diabetes.select_dtypes(include=['int64', 'float64'])

# Select the nominal (categorical) columns
nominal_cols = diabetes.select_dtypes(include=['object'])

# Function to plot histogram with annotations
def plot_histogram_with_annotations(col_data, col_name, bins=20):
    plt.figure(figsize=(10, 6))
    sns_histplot = sns.histplot(col_data, bins=bins, kde=False, color='skyblue', alpha=0.7)
    plt.title(f'Histogram of {col_name}')
    plt.xlabel(col_name)
    plt.ylabel('Frequency')

    # Annotate histogram
    for p in sns_histplot.patches:
        height = p.get_height()
        if height > 0:  # Add annotation only if height is greater than 0
            sns_histplot.annotate(format(height, '.0f'),
                                  (p.get_x() + p.get_width() / 2., height),
                                  ha='center', va='center',
                                  xytext=(0, 10),
                                  textcoords='offset points',
                                  fontsize=10, color='black', rotation=0)

    # Adjust y-limit to make space for annotations
    max_height = max([p.get_height() for p in sns_histplot.patches])
    plt.ylim(0, max_height * 1.3)  # Adjust to provide more space

    plt.show()

# Plot histogram for numeric data with annotations
for col in numerical_cols.columns:
    if col in ['age', 'bmi']:  # Adjusted to match diabetes column names
        plot_histogram_with_annotations(numerical_cols[col], col, bins=30)  # More bins for detailed data
    else:
        plot_histogram_with_annotations(numerical_cols[col], col)

# Plot bar chart for nominal data with annotations
for col in nominal_cols.columns:
    plt.figure(figsize=(10, 6))
    value_counts = nominal_cols[col].value_counts()
    # Use catplot to handle palettes and avoid warnings
    barplot = sns.catplot(x=value_counts.index, y=value_counts.values, kind='bar', palette="viridis", height=6, aspect=1.5)
    plt.title(f'Bar Chart of {col}')
    plt.xlabel(col)
    plt.ylabel('Count')
    plt.xticks(rotation=0)

    # Annotate bars with counts
    for ax in barplot.axes.flat:
        for p in ax.patches:
            height = p.get_height()
            if height > 0:  # Add annotation only if height is greater than 0
                ax.annotate(format(height, '.0f'),
                            (p.get_x() + p.get_width() / 2., height),
                            ha='center', va='center',
                            xytext=(0, 10),
                            textcoords='offset points',
                            fontsize=10, color='black', rotation=0)

    plt.tight_layout()
    plt.show()

diabetes = pd.get_dummies(diabetes)
diabetes.head()

diabetes.info()

diabetes_values = diabetes['diabetes'].value_counts()
diabetes_values

X = diabetes.drop('diabetes', axis=1)
y = diabetes['diabetes']

X.info()

from sklearn.preprocessing import StandardScaler
scaler = StandardScaler()
X = scaler.fit_transform(X)

from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=5)

print("Training Data Size: ", X_train.shape, y_train.shape)
print("Testing Data Size: ", X_test.shape,y_test.shape)
print("")
print("Target Column Before SMOTE")
print("------------------------------------------")
print(y_train.value_counts())

from imblearn.over_sampling import SMOTE
X_train_resample, y_train_resample = SMOTE(random_state=5).fit_resample(X_train, y_train)

X_train_resample

print("Target Column After SMOTE")
print("--------------------------------------")
print(y_train_resample.value_counts())

from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score
from sklearn.linear_model import LogisticRegression
from sklearn.neighbors import KNeighborsClassifier
from sklearn.ensemble import GradientBoostingClassifier, RandomForestClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.naive_bayes import BernoulliNB

lr = LogisticRegression(random_state=5)
knn = KNeighborsClassifier()
gb = GradientBoostingClassifier(random_state=5)
rf = RandomForestClassifier(random_state=5)
dt = DecisionTreeClassifier(random_state=5)
nb = BernoulliNB()

lr.fit(X_train_resample, y_train_resample)
knn.fit(X_train_resample, y_train_resample)
gb.fit(X_train_resample, y_train_resample)
rf.fit(X_train_resample, y_train_resample)
dt.fit(X_train_resample, y_train_resample)
nb.fit(X_train_resample, y_train_resample)

y_pred1 = lr.predict(X_test)
y_pred2 = knn.predict(X_test)
y_pred3 = gb.predict(X_test)
y_pred4 = rf.predict(X_test)
y_pred5 = dt.predict(X_test)
y_pred6 = nb.predict(X_test)

import numpy as np
import pandas as pd
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC
from sklearn.linear_model import LogisticRegression
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.tree import DecisionTreeClassifier
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import roc_curve, auc
from sklearn.naive_bayes import BernoulliNB
import matplotlib.pyplot as plt

# Initialize the models
models = {
    'Logistic Regression': LogisticRegression(random_state=5),
    'Random Forest': RandomForestClassifier(random_state=5),
    'Decision Tree': DecisionTreeClassifier(random_state=5),
    'Gradient Boosting': GradientBoostingClassifier(random_state=5),
    'KNN': KNeighborsClassifier(),
    'Bernoulli NB' : BernoulliNB()
}

# Fit the models and predict probabilities
probas = {}
for model_name, model in models.items():
    model.fit(X_train_resample, y_train_resample)
    probas[model_name] = model.predict_proba(X_test)[:, 1]

# Plot ROC curves
#plt.figure(figsize=(9, 7))

for model_name, y_proba in probas.items():
    fpr, tpr, _ = roc_curve(y_test, y_proba)
    roc_auc = auc(fpr, tpr)
    plt.plot(fpr, tpr, lw=2, label=f'{model_name} (AUC = {roc_auc:.2f})')

plt.plot([0, 1], [0, 1], color='navy', lw=2, linestyle='--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.0])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve of Different Classification Models')
plt.legend(loc="lower right")
plt.tight_layout()
plt.show()

import numpy as np
import pandas as pd
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score

# Initialize the models
models = {
    'Logistic Regression': LogisticRegression(random_state=5),
    'Random Forest': RandomForestClassifier(random_state=5),
    'Decision Tree': DecisionTreeClassifier(random_state=5),
    'Gradient Boosting': GradientBoostingClassifier(random_state=5),
    'KNN': KNeighborsClassifier(),
    'Bernoulli NB': BernoulliNB()
}

# Store evaluation metrics
metrics = {
    'Model': [],
    'Accuracy': [],
    'Precision': [],
    'Recall': [],
    'F1-Score': [],
    'AUC-ROC': []
}

# Fit models and calculate metrics
for model_name, model in models.items():
    model.fit(X_train_resample, y_train_resample)
    y_pred = model.predict(X_test)
    y_proba = model.predict_proba(X_test)[:, 1]

    metrics['Model'].append(model_name)
    metrics['Accuracy'].append(round(accuracy_score(y_test, y_pred), 2))
    metrics['Precision'].append(round(precision_score(y_test, y_pred), 2))
    metrics['Recall'].append(round(recall_score(y_test, y_pred), 2))
    metrics['F1-Score'].append(round(f1_score(y_test, y_pred), 2))
    metrics['AUC-ROC'].append(round(roc_auc_score(y_test, y_proba), 2))

# Create a DataFrame
metrics_df = pd.DataFrame(metrics)

# Display the DataFrame
metrics_df

# Initialize the GradientBoostingClassifier
gb = GradientBoostingClassifier(random_state=5)

# Fit the model with the resampled training data
gb.fit(X_train_resample, y_train_resample)

# Function to preprocess the user input
def preprocess_input(gender, smoking_history, age, hypertension, heart_disease, bmi, hbA1c_level, blood_glucose_level):
    input_data = pd.DataFrame({
        'age': [age],
        'hypertension': [hypertension],
        'heart_disease': [heart_disease],
        'bmi': [bmi],
        'HbA1c_level': [hbA1c_level],
        'blood_glucose_level': [blood_glucose_level]
    })

    # One-hot encoding for gender
    gender_encoded = pd.get_dummies([gender], prefix='gender', drop_first=True)

    # One-hot encoding for smoking history
    smoking_history_encoded = pd.get_dummies([smoking_history], prefix='smoking_history', drop_first=True)

    # Concatenate encoded gender and smoking history with the user input
    input_data = pd.concat([input_data, gender_encoded, smoking_history_encoded], axis=1)

    # Ensure that all columns are present
    expected_columns = [
        'age', 'hypertension', 'heart_disease', 'bmi', 'HbA1c_level', 'blood_glucose_level',
        'gender_Female', 'gender_Male', 'gender_Other',
        'smoking_history_No Info', 'smoking_history_current', 'smoking_history_ever',
        'smoking_history_former', 'smoking_history_never', 'smoking_history_not current'
    ]

    for col in expected_columns:
        if col not in input_data.columns:
            input_data[col] = 0

    # Reorder columns to match the model's expected input
    input_data = input_data[expected_columns]

    # Scale the input data (use the same scaler as training)
    input_data_scaled = scaler.transform(input_data)

    return input_data_scaled

# Function to make a prediction
def make_prediction():
    while True:
        print('##############################')
        print("#### DIABETES PREDICTION ####")
        print('##############################')
        print(" ")
        print("Please enter the following details:")
        print("------------------------------------------------------------------------")
        gender = input("Gender (Male/Female/Other): ")
        smoking_history = input("Smoking History (current/ever/former/never/No Info/not current): ")
        age = int(input("Age: "))
        hypertension = int(input("Hypertension (1 for Yes, 0 for No): "))
        heart_disease = int(input("Heart Disease (1 for Yes, 0 for No): "))
        bmi = float(input("BMI: "))
        hbA1c_level = float(input("HbA1c Level: "))
        blood_glucose_level = float(input("Blood Glucose Level: "))
        print("------------------------------------------------------------------------")
        # Preprocess the input data
        input_data = preprocess_input(gender, smoking_history, age, hypertension, heart_disease, bmi, hbA1c_level, blood_glucose_level)

        # Make a prediction
        prediction = gb.predict(input_data)

        # Output the result
        print("******************************************************************")
        print("Diabetes Prediction:", "YES !!" if prediction[0] == 1 else "No")
        print("******************************************************************")
        # Ask if the user wants to continue
        print("-------------------------------------------------------------------")
        continue_prediction = input("Do you want to make another prediction? (yes/no): ").strip().lower()
        print("-------------------------------------------------------------------")
        if continue_prediction != 'yes':
            print("Exiting the prediction tool. Goodbye!")
            break

# Call the function to take input and make predictions
make_prediction()